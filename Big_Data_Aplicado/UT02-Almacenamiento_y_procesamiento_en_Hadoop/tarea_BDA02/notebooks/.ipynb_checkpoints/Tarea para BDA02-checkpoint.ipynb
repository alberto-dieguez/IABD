{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plantilla para la Tarea online BDA02\n",
    "\n",
    "# Alberto Diéguez Álvarez:\n",
    "\n",
    "En esta tarea deberás completar las celdas que están incompletas. Se muestra el resultado esperado de la ejecución. Se trata de que implementes un proceso MapReduce que produzca ese resultado. Puedes implementar el proceso MapReduce con el lenguaje y librería que prefieras (`Bash`, Python, `mrjob` ...). Los datos de entrada del proceso son meros ejemplos y el proceso que implementes debería funcionar con esos y cualquier otro fichero de entrada que tenga la misma estructura."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.- Partiendo del fichero de `notas.txt`, calcula la nota más alta obtenida por cada alumno con un proceso MapReduce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es decir, que si tenemos el fichero de notas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting notas.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile notas.txt\n",
    "pedro 6 7\n",
    "luis 0 4\n",
    "ana 7\n",
    "pedro 8 1 3\n",
    "ana 5 6 7\n",
    "ana 10\n",
    "luis 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se espera obtener el siguiente resultado:\n",
    "\n",
    "![solución 1](./img/1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leemos cada línea, extraemos el nombre y tratamos la nota."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapperNotas.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapperNotas.py\n",
    "#!/usr/bin/python3\n",
    "\n",
    "import sys\n",
    "\n",
    "# Leemos línea a línea de la entrada estándar\n",
    "for line in sys.stdin:  \n",
    "    # Extraemos el nombre y las notas\n",
    "    name, *marks = line.split()\n",
    "    marks = list(map(int, marks))\n",
    "    \n",
    "    # Emitimos cada nota con el nombre del estudiante como clave\n",
    "    for mark in marks:\n",
    "        print(f'{name}\\t{mark}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod ugo+x mapperNotas.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pedro\t6\r\n",
      "pedro\t7\r\n",
      "luis\t0\r\n",
      "luis\t4\r\n",
      "ana\t7\r\n",
      "pedro\t8\r\n",
      "pedro\t1\r\n",
      "pedro\t3\r\n",
      "ana\t5\r\n",
      "ana\t6\r\n",
      "ana\t7\r\n",
      "ana\t10\r\n",
      "luis\t3\r\n"
     ]
    }
   ],
   "source": [
    "! cat notas.txt | ./mapperNotas.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el siguiente script comprobamos que el nombre no se repite y que la nota es la más alta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducerNotas.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducerNotas.py\n",
    "#!/usr/bin/python3\n",
    "\n",
    "import sys\n",
    "\n",
    "prev_name=''\n",
    "max_mark = -1\n",
    "\n",
    "# Leemos línea a línea de la entrada estándar\n",
    "for line in sys.stdin: \n",
    "    \n",
    "    name, mark = line.split()\n",
    "    mark = float(mark)\n",
    "    \n",
    "    # Si el nombre es igual al de la anterior línea o es la primera iteración, acumulamos la suma de notas y el nḿero de notas\n",
    "    if not prev_name or prev_name == name:                \n",
    "        max_mark  = max(max_mark, mark)\n",
    "    \n",
    "    # Cuando el nombre sea diferente, emitimos el nombre anterior y la nota más alta\n",
    "    else:\n",
    "        print(f'\"{prev_name}\"\\t{max_mark:.1f}')\n",
    "        max_mark = mark\n",
    "        \n",
    "    prev_name=name\n",
    "           \n",
    "# Emitimos el nombre y la nota más alta del último nombre\n",
    "print(f'\"{prev_name}\"\\t{max_mark:.1f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod ugo+x reducerNotas.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"ana\"\t10.0\r\n",
      "\"luis\"\t4.0\r\n",
      "\"pedro\"\t8.0\r\n"
     ]
    }
   ],
   "source": [
    "! cat notas.txt | ./mapperNotas.py | sort | ./reducerNotas.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En los siguientes dos scripts lo que hago es borrar los archivos, así al ejecutar el libro entero no me da error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/root/notas.txt\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -rm /user/root/notas.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/root/output\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -rm -r /user/root/output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hadoop fs -copyFromLocal notas.txt /user/root/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   3 root supergroup         61 2025-01-07 16:13 /user/root/notas.txt\r\n",
      "drwxr-xr-x   - root supergroup          0 2025-01-07 11:14 /user/root/tmp\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -ls /user/root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [] [/app/hadoop-3.3.1/share/hadoop/tools/lib/hadoop-streaming-3.3.1.jar] /tmp/streamjob3439386721766760603.jar tmpDir=null\n",
      "2025-01-07 16:13:19,557 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmaster/172.20.0.2:8032\n",
      "2025-01-07 16:13:19,633 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmaster/172.20.0.2:8032\n",
      "2025-01-07 16:13:19,732 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1736255952742_0035\n",
      "2025-01-07 16:13:19,878 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2025-01-07 16:13:19,906 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "2025-01-07 16:13:19,968 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1736255952742_0035\n",
      "2025-01-07 16:13:19,968 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2025-01-07 16:13:20,055 INFO conf.Configuration: resource-types.xml not found\n",
      "2025-01-07 16:13:20,055 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2025-01-07 16:13:20,081 INFO impl.YarnClientImpl: Submitted application application_1736255952742_0035\n",
      "2025-01-07 16:13:20,097 INFO mapreduce.Job: The url to track the job: http://yarnmaster:8088/proxy/application_1736255952742_0035/\n",
      "2025-01-07 16:13:20,098 INFO mapreduce.Job: Running job: job_1736255952742_0035\n",
      "2025-01-07 16:13:24,151 INFO mapreduce.Job: Job job_1736255952742_0035 running in uber mode : false\n",
      "2025-01-07 16:13:24,152 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2025-01-07 16:13:27,182 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2025-01-07 16:13:31,202 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2025-01-07 16:13:31,206 INFO mapreduce.Job: Job job_1736255952742_0035 completed successfully\n",
      "2025-01-07 16:13:31,251 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=124\n",
      "\t\tFILE: Number of bytes written=832123\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=276\n",
      "\t\tHDFS: Number of bytes written=34\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=2644\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1229\n",
      "\t\tTotal time spent by all map tasks (ms)=2644\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1229\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=2644\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1229\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=2707456\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1258496\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=7\n",
      "\t\tMap output records=13\n",
      "\t\tMap output bytes=92\n",
      "\t\tMap output materialized bytes=130\n",
      "\t\tInput split bytes=184\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=3\n",
      "\t\tReduce shuffle bytes=130\n",
      "\t\tReduce input records=13\n",
      "\t\tReduce output records=3\n",
      "\t\tSpilled Records=26\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=90\n",
      "\t\tCPU time spent (ms)=780\n",
      "\t\tPhysical memory (bytes) snapshot=798216192\n",
      "\t\tVirtual memory (bytes) snapshot=7656587264\n",
      "\t\tTotal committed heap usage (bytes)=732954624\n",
      "\t\tPeak Map Physical memory (bytes)=298024960\n",
      "\t\tPeak Map Virtual memory (bytes)=2549145600\n",
      "\t\tPeak Reduce Physical memory (bytes)=202678272\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2558640128\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=92\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=34\n",
      "2025-01-07 16:13:31,251 INFO streaming.StreamJob: Output directory: /user/root/output\n"
     ]
    }
   ],
   "source": [
    "! mapred streaming \\\n",
    "    -files /media/notebooks/mapperNotas.py,/media/notebooks/reducerNotas.py \\\n",
    "    -input /user/root/notas.txt \\\n",
    "    -output /user/root/output \\\n",
    "    -mapper mapperNotas.py \\\n",
    "    -reducer reducerNotas.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"ana\"\t10.0\r\n",
      "\"luis\"\t4.0\r\n",
      "\"pedro\"\t8.0\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -cat /user/root/output/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Añado una foto con la salida del script.\n",
    "![image](./img/alberto_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.- Usando un proceso MapReduce muestra las 10 palabras más utilizadas en `El Quijote`.\n",
    "\n",
    "Lo primero será descargar El Quijote:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-01-07 16:13:33--  https://www.gutenberg.org/files/2000/2000-0.txt\n",
      "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2226045 (2.1M) [text/plain]\n",
      "Saving to: ‘2000-0.txt’\n",
      "\n",
      "2000-0.txt          100%[===================>]   2.12M  2.47MB/s    in 0.9s    \n",
      "\n",
      "2025-01-07 16:13:34 (2.47 MB/s) - ‘2000-0.txt’ saved [2226045/2226045]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget -O '2000-0.txt' https://www.gutenberg.org/files/2000/2000-0.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al igual que hicimos en la primera práctica, eliminamos aquellas líneas que son metadata y no forman parte de la obra. Sobrescribimos el fichero sin esas líneas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('2000-0.txt') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "head = 24\n",
    "tail = 360\n",
    "book = lines[head:-tail]\n",
    "\n",
    "with open('2000-0.txt', 'w') as f:\n",
    "    for line in book:\n",
    "        f.write(f\"{line}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El resultado debería ser el mismo que el que obtuvimos en la primera práctica.\n",
    "\n",
    "![solución 2](./img/2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos el mapper que inicia el contador, convierte a minúsculas y al final muestra la frecuencia de las palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapperWords.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapperWords.py\n",
    "#!/usr/bin/python3\n",
    "\n",
    "import sys\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Inicializamos un contador para las palabras\n",
    "word_count = Counter()\n",
    "\n",
    "# Leemos línea por línea de la entrada estándar\n",
    "for line in sys.stdin:\n",
    "    # Convertir la línea a minúsculas y eliminar caracteres no alfabéticos\n",
    "    line = re.sub(r'[^a-zA-Z\\s]', '', line.lower())\n",
    "    # Dividir la línea en palabras\n",
    "    words = line.split()\n",
    "    \n",
    "    # Actualizamos el contador de palabras\n",
    "    word_count.update(words)\n",
    "\n",
    "# Después de procesar todas las líneas, emitimos la frecuencia de las palabras\n",
    "for word, count in word_count.items():\n",
    "    print(f'{count}\\t{word}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod ugo+x mapperWords.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hago esto en el código porque si no me da una lista enorme. Hago esto solo para este ejemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8265\tel\r\n",
      "29\tingenioso\r\n",
      "76\thidalgo\r\n",
      "2714\tdon\r\n",
      "2241\tquijote\r\n"
     ]
    }
   ],
   "source": [
    "! cat 2000-0.txt | ./mapperWords.py 2>/dev/null | head -n 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos el reduce que acumula la frecuencia de las palabras y las ordena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducerWords.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducerWords.py\n",
    "#!/usr/bin/python3\n",
    "import sys\n",
    "\n",
    "# Creamos un diccionario para almacenar las palabras y sus frecuencias\n",
    "word_count = {}\n",
    "\n",
    "# Leemos línea a línea de la entrada estándar\n",
    "for line in sys.stdin:\n",
    "    # Limpiamos los espacios en blanco antes y después de la línea\n",
    "    line = line.strip()\n",
    "    \n",
    "    # Si la línea no está vacía, procesamos\n",
    "    if line:\n",
    "        try:\n",
    "            count, word = line.split(\"\\t\")\n",
    "            count = int(count)\n",
    "            \n",
    "            # Acumulamos la frecuencia de cada palabra\n",
    "            if word in word_count:\n",
    "                word_count[word] += count\n",
    "            else:\n",
    "                word_count[word] = count\n",
    "        except ValueError:\n",
    "            print(word_count)\n",
    "            continue\n",
    "\n",
    "# Convertimos las palabras y frecuencias a una lista de tuplas\n",
    "sorted_words = sorted(word_count.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Imprimimos la salida en el formato esperado: lista de pares [frecuencia, palabra]\n",
    "output = \"[\"\n",
    "output += \", \".join([f\"[{freq}, '{word}']\" for word, freq in sorted_words[:10]])\n",
    "output += \"]\"\n",
    "print(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod ugo+x reducerWords.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20767, 'que'], [18410, 'de'], [18271, 'y'], [10492, 'la'], [9875, 'a'], [8284, 'en'], [8265, 'el'], [6346, 'no'], [4769, 'los'], [4768, 'se']]\r\n"
     ]
    }
   ],
   "source": [
    "! cat 2000-0.txt | ./mapperWords.py | sort | ./reducerWords.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/root/2000-0.txt': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -rm /user/root/2000-0.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/root/output\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -rm -r /user/root/output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hadoop fs -copyFromLocal 2000-0.txt /user/root/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\r\n",
      "-rw-r--r--   3 root supergroup    2205995 2025-01-07 16:13 /user/root/2000-0.txt\r\n",
      "-rw-r--r--   3 root supergroup         61 2025-01-07 16:13 /user/root/notas.txt\r\n",
      "drwxr-xr-x   - root supergroup          0 2025-01-07 11:14 /user/root/tmp\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -ls /user/root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [] [/app/hadoop-3.3.1/share/hadoop/tools/lib/hadoop-streaming-3.3.1.jar] /tmp/streamjob4200082116501785518.jar tmpDir=null\n",
      "2025-01-07 16:33:27,678 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmaster/172.20.0.2:8032\n",
      "2025-01-07 16:33:27,755 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmaster/172.20.0.2:8032\n",
      "2025-01-07 16:33:27,855 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1736255952742_0045\n",
      "2025-01-07 16:33:27,994 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2025-01-07 16:33:28,019 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "2025-01-07 16:33:28,085 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1736255952742_0045\n",
      "2025-01-07 16:33:28,085 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2025-01-07 16:33:28,173 INFO conf.Configuration: resource-types.xml not found\n",
      "2025-01-07 16:33:28,173 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2025-01-07 16:33:28,200 INFO impl.YarnClientImpl: Submitted application application_1736255952742_0045\n",
      "2025-01-07 16:33:28,216 INFO mapreduce.Job: The url to track the job: http://yarnmaster:8088/proxy/application_1736255952742_0045/\n",
      "2025-01-07 16:33:28,217 INFO mapreduce.Job: Running job: job_1736255952742_0045\n",
      "2025-01-07 16:33:32,257 INFO mapreduce.Job: Job job_1736255952742_0045 running in uber mode : false\n",
      "2025-01-07 16:33:32,258 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2025-01-07 16:33:35,284 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2025-01-07 16:33:39,311 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2025-01-07 16:33:39,316 INFO mapreduce.Job: Job job_1736255952742_0045 completed successfully\n",
      "2025-01-07 16:33:39,361 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=385257\n",
      "\t\tFILE: Number of bytes written=1602395\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2210277\n",
      "\t\tHDFS: Number of bytes written=146\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=3013\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1310\n",
      "\t\tTotal time spent by all map tasks (ms)=3013\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1310\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=3013\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1310\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=3085312\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1341440\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=75356\n",
      "\t\tMap output records=30600\n",
      "\t\tMap output bytes=324051\n",
      "\t\tMap output materialized bytes=385263\n",
      "\t\tInput split bytes=186\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=367\n",
      "\t\tReduce shuffle bytes=385263\n",
      "\t\tReduce input records=30600\n",
      "\t\tReduce output records=1\n",
      "\t\tSpilled Records=61200\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=104\n",
      "\t\tCPU time spent (ms)=1530\n",
      "\t\tPhysical memory (bytes) snapshot=876544000\n",
      "\t\tVirtual memory (bytes) snapshot=7655108608\n",
      "\t\tTotal committed heap usage (bytes)=771751936\n",
      "\t\tPeak Map Physical memory (bytes)=337580032\n",
      "\t\tPeak Map Virtual memory (bytes)=2549248000\n",
      "\t\tPeak Reduce Physical memory (bytes)=201900032\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2557980672\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2210091\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=146\n",
      "2025-01-07 16:33:39,361 INFO streaming.StreamJob: Output directory: /user/root/output\n"
     ]
    }
   ],
   "source": [
    "! mapred streaming \\\n",
    "    -files /media/notebooks/mapperWords.py,/media/notebooks/reducerWords.py \\\n",
    "    -input /user/root/2000-0.txt \\\n",
    "    -output /user/root/output \\\n",
    "    -mapper mapperWords.py \\\n",
    "    -reducer reducerWords.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20767, 'que'], [18410, 'de'], [18271, 'y'], [10492, 'la'], [9875, 'a'], [8284, 'en'], [8265, 'el'], [6346, 'no'], [4769, 'los'], [4768, 'se']]\t\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -cat /user/root/output/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Añado una foto con la salida del script.\n",
    "![image](./img/alberto_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.- Muestra la clasificación de temporada 2021/2022 de La Liga pero únicamente de los puntos obtenidos como visitante.\n",
    "\n",
    "En [esta Web](https://resultados.as.com/resultados/futbol/primera/2021_2022/clasificacion/) puedes consultar cuántos puntos obtuvo cada equipo fuera de casa.\n",
    "\n",
    "Empezamos descargando el fichero de resultados de la temporada 2021/2022 y renombrándolo a `laliga2122.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-01-07 16:13:57--  https://www.football-data.co.uk/mmz4281/2122/SP1.csv\n",
      "Resolving www.football-data.co.uk (www.football-data.co.uk)... 217.160.0.246\n",
      "Connecting to www.football-data.co.uk (www.football-data.co.uk)|217.160.0.246|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 172174 (168K) [text/csv]\n",
      "Saving to: ‘laliga2122.csv’\n",
      "\n",
      "laliga2122.csv      100%[===================>] 168.14K  1.09MB/s    in 0.2s    \n",
      "\n",
      "2025-01-07 16:13:57 (1.09 MB/s) - ‘laliga2122.csv’ saved [172174/172174]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget -O laliga2122.csv https://www.football-data.co.uk/mmz4281/2122/SP1.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se espera este resultado:\n",
    "\n",
    "![solución 3](./img/3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/root/2000-0.txt\r\n",
      "Deleted /user/root/notas.txt\r\n",
      "Deleted /user/root/output\r\n",
      "Deleted /user/root/tmp\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -rm -r /user/root/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostramos las dos primeras líneas del fichero. Observa que la primera línea es la cabecera y la siguiente es la información sobre un partido de fútbol. Ambas líneas tienen los campos separados por comas (es lo que significa csv: \"comma-separated values\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Div,Date,Time,HomeTeam,AwayTeam,FTHG,FTAG,FTR,HTHG,HTAG,HTR,HS,AS,HST,AST,HF,AF,HC,AC,HY,AY,HR,AR,B365H,B365D,B365A,BWH,BWD,BWA,IWH,IWD,IWA,PSH,PSD,PSA,WHH,WHD,WHA,VCH,VCD,VCA,MaxH,MaxD,MaxA,AvgH,AvgD,AvgA,B365>2.5,B365<2.5,P>2.5,P<2.5,Max>2.5,Max<2.5,Avg>2.5,Avg<2.5,AHh,B365AHH,B365AHA,PAHH,PAHA,MaxAHH,MaxAHA,AvgAHH,AvgAHA,B365CH,B365CD,B365CA,BWCH,BWCD,BWCA,IWCH,IWCD,IWCA,PSCH,PSCD,PSCA,WHCH,WHCD,WHCA,VCCH,VCCD,VCCA,MaxCH,MaxCD,MaxCA,AvgCH,AvgCD,AvgCA,B365C>2.5,B365C<2.5,PC>2.5,PC<2.5,MaxC>2.5,MaxC<2.5,AvgC>2.5,AvgC<2.5,AHCh,B365CAHH,B365CAHA,PCAHH,PCAHA,MaxCAHH,MaxCAHA,AvgCAHH,AvgCAHA\r",
      "\r\n",
      "SP1,13/08/2021,20:00,Valencia,Getafe,1,0,H,1,0,H,4,22,2,4,24,15,1,9,6,3,1,1,2.55,3,3.1,2.65,3,2.95,2.65,2.9,3.05,2.7,3.03,3.11,2.55,3,3,2.63,3,3,2.73,3.2,3.23,2.64,3.01,3.06,2.62,1.5,2.75,1.5,2.75,1.51,2.65,1.49,0,1.82,2.11,1.83,2.11,1.88,2.13,1.81,2.08,2.37,3,3.3,2.45,3,3.25,2.4,2.95,3.4,2.47,3.04,3.48,2.35,3,3.3,2.45,3,3.3,2.57,3.1,3.58,2.42,3,3.34,2.75,1.44,2.84,1.48,2.84,1.51,2.68,1.47,-0.25,2.06,1.87,2.07,1.86,2.1,1.9,2.03,1.84\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "! head -2 laliga2122.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para entender el significado de cada campo, la Web tiene un fichero de [metadata](https://www.football-data.co.uk/notes.txt). En la imagen se muestran los campos relevantes para el proceso que queremos realizar.\n",
    "\n",
    "Concretamente, los campos 4º y 5º contienen los nombres de los equipos local y visitante respectivamente y el campo 8º informa cuál de ellos obtuvo la victoria. Así 'H' significa que ganó el equipo local, 'A' que lo hizo el visitante y 'D' que empataron. Sabiendo que el equipo que gana obtiene 3 puntos, el que pierde 0 puntos y si empatan ambos equipos se llevan 1 punto, podemos calcular la clasificación final de la liga.\n",
    "\n",
    "Este ejercicio lo vamos a resolver únicamente con mrjob. En este caso hemos tenido que hacer uso del método steps de mrjob que permite definir etapas. En este ejercicio es necesario ya que tenemos dos reductores, uno para calcular la suma de los puntos de un equipo y otro para calcular la clasificación. Esta es una descripción del proceso MapReduce:\n",
    "\n",
    "La primera etapa comienza con el mapper al que hemos llamado mapper_points que lo que hace es procesar cada línea que corresponde a un partido y extraer los equipos que se enfrentan. Emite como clave el nombre del equipo y como valor los puntos que ha obtenido.\n",
    "El combiner_points es un combinador. Es un proceso que hace una función parecida al reductor y que permite optimizar el funcionamiento ya que hace agregaciones parciales e intermedias antes de enviarlas al reductor.\n",
    "El reducer_points recibe como clave cada equipo y como valor un iterador con los puntos que ha obtenido ese equipo. Emite como clave None y como valor una tupla que contiene el nombre del equipo y la suma de los puntos. Al emitir una clave None todos las tuplas emitidas serán procesadas en un único reductor en la próxima etapa. Es muy importante asegurar que el volumen de datos que reciba ese reductor sea pequeño.\n",
    "La segunda etapa sólo consta de un reductor llamado reducer_classification. Este reductor ignora la clave ya que no contiene información útil y como valor recibe un iterador de tuplas equipo,puntos emitido por el reductor de la primera etapa, reducer_points. Lo que hace es emitir una clave nula con los equipos ordenados por puntos de mayor a menor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos el script en python con mapper, combiner y reducer.\n",
    "Simplemente sumamos los puntos si el equipo es visitante. Luego los ordenamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting laligaMR.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile laligaMR.py\n",
    "#!/usr/bin/python3\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "    \n",
    "class LaLigaMR(MRJob):\n",
    "        \n",
    "    # Mapper: En esta etapa aún no hay clave (_), el valor lo recibimos en la variable line\n",
    "    def mapper_points(self, _, line):\n",
    "        #Por cada línea, esta se divide en los campos que forman las columnas\n",
    "        _, _, _, home_team, away_team, _, _, result, *rest = line.split(',')\n",
    "        \n",
    "        # Si es la cabecera no emitimos nada\n",
    "        if home_team == \"HomeTeam\":\n",
    "            return\n",
    "        \n",
    "        if result == 'D':            \n",
    "            yield away_team, 1\n",
    "        elif result == 'H':\n",
    "            yield away_team, 0\n",
    "        else:\n",
    "            yield away_team, 3\n",
    "            \n",
    "    def combiner_points(self, team, points):\n",
    "        yield team, sum(points)\n",
    "            \n",
    "    def reducer_points(self, team, points):\n",
    "        yield None, (team, sum(points))\n",
    "        \n",
    "    def reducer_classification(self, _, points):\n",
    "        yield None, sorted(points, key=lambda t: t[1], reverse=True)\n",
    "        \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_points,\n",
    "                   combiner=self.combiner_points,\n",
    "                   reducer=self.reducer_points),\n",
    "            MRStep(reducer=self.reducer_classification)\n",
    "        ]\n",
    "         \n",
    "if __name__=='__main__':\n",
    "    LaLigaMR.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod ugo+x laligaMR.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /app/hadoop-3.3.1/bin...\n",
      "Found hadoop binary: /app/hadoop-3.3.1/bin/hadoop\n",
      "Using Hadoop version 3.3.1\n",
      "Looking for Hadoop streaming jar in /app/hadoop-3.3.1...\n",
      "Found Hadoop streaming jar: /app/hadoop-3.3.1/share/hadoop/tools/lib/hadoop-streaming-3.3.1.jar\n",
      "Creating temp directory /tmp/laligaMR.root.20250107.153847.801214\n",
      "uploading working dir files to hdfs:///user/root/tmp/mrjob/laligaMR.root.20250107.153847.801214/files/wd...\n",
      "Copying other local files to hdfs:///user/root/tmp/mrjob/laligaMR.root.20250107.153847.801214/files/\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [/tmp/hadoop-unjar6735978285436285139/] [] /tmp/streamjob1510403203801731.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.20.0.2:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.20.0.2:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1736255952742_0046\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1736255952742_0046\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1736255952742_0046\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1736255952742_0046/\n",
      "  Running job: job_1736255952742_0046\n",
      "  Job job_1736255952742_0046 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1736255952742_0046 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/laligaMR.root.20250107.153847.801214/step-output/0000\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=176270\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=428\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=608\n",
      "\t\tFILE: Number of bytes written=836826\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=176570\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=428\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=3076096\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1360896\n",
      "\t\tTotal time spent by all map tasks (ms)=3004\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=3004\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1329\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1329\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=3004\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1329\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=880\n",
      "\t\tCombine input records=380\n",
      "\t\tCombine output records=40\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=99\n",
      "\t\tInput split bytes=300\n",
      "\t\tMap input records=381\n",
      "\t\tMap output bytes=4750\n",
      "\t\tMap output materialized bytes=614\n",
      "\t\tMap output records=380\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=337731584\n",
      "\t\tPeak Map Virtual memory (bytes)=2549899264\n",
      "\t\tPeak Reduce Physical memory (bytes)=200478720\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2558562304\n",
      "\t\tPhysical memory (bytes) snapshot=834916352\n",
      "\t\tReduce input groups=20\n",
      "\t\tReduce input records=40\n",
      "\t\tReduce output records=20\n",
      "\t\tReduce shuffle bytes=614\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=80\n",
      "\t\tTotal committed heap usage (bytes)=752877568\n",
      "\t\tVirtual memory (bytes) snapshot=7657009152\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 2 of 2...\n",
      "  packageJobJar: [/tmp/hadoop-unjar8871838669053462187/] [] /tmp/streamjob689367319502179976.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.20.0.2:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.20.0.2:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1736255952742_0047\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1736255952742_0047\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1736255952742_0047\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1736255952742_0047/\n",
      "  Running job: job_1736255952742_0047\n",
      "  Job job_1736255952742_0047 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1736255952742_0047 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/laligaMR.root.20250107.153847.801214/output\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=642\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=354\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=474\n",
      "\t\tFILE: Number of bytes written=835022\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=956\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=354\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=2685952\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1305600\n",
      "\t\tTotal time spent by all map tasks (ms)=2623\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=2623\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1275\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1275\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=2623\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1275\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=740\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=134\n",
      "\t\tInput split bytes=314\n",
      "\t\tMap input records=20\n",
      "\t\tMap output bytes=428\n",
      "\t\tMap output materialized bytes=480\n",
      "\t\tMap output records=20\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=304566272\n",
      "\t\tPeak Map Virtual memory (bytes)=2549862400\n",
      "\t\tPeak Reduce Physical memory (bytes)=202645504\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2556305408\n",
      "\t\tPhysical memory (bytes) snapshot=806572032\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce input records=20\n",
      "\t\tReduce output records=1\n",
      "\t\tReduce shuffle bytes=480\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=40\n",
      "\t\tTotal committed heap usage (bytes)=735051776\n",
      "\t\tVirtual memory (bytes) snapshot=7655141376\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///user/root/tmp/mrjob/laligaMR.root.20250107.153847.801214/output\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/laligaMR.root.20250107.153847.801214/output...\n",
      "null\t[[\"Real Madrid\", 42], [\"Barcelona\", 35], [\"Betis\", 33], [\"Ath Madrid\", 30], [\"Sevilla\", 28], [\"Sociedad\", 27], [\"Osasuna\", 25], [\"Villarreal\", 23], [\"Valencia\", 22], [\"Celta\", 21], [\"Cadiz\", 21], [\"Ath Bilbao\", 21], [\"Granada\", 16], [\"Elche\", 15], [\"Vallecano\", 13], [\"Levante\", 13], [\"Mallorca\", 12], [\"Getafe\", 11], [\"Espanol\", 9], [\"Alaves\", 6]]\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/laligaMR.root.20250107.153847.801214...\n",
      "Removing temp directory /tmp/laligaMR.root.20250107.153847.801214...\n"
     ]
    }
   ],
   "source": [
    "! python3 laligaMR.py -r hadoop laliga2122.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.- Muestra la diferencia de goles entre el equipo que más goles ha marcado y el que menos goles ha marcado en la temporada 2021/2022 de La Liga.\n",
    "\n",
    "Se espera que el proceso MapReuce produzca una salida similar a la siguiente:\n",
    "\n",
    "![solución 4](./img/4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos el script. Sumamos todos los goles, tanto marcados como visitanto como jugando en casa.\n",
    "Los ordenamos, hallamos la diferencia y mostramos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing laligaMRDiff.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile laligaMRDiff.py\n",
    "#!/usr/bin/python3\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class LaLigaMR(MRJob):\n",
    "        \n",
    "    # Mapper: Extraemos goles de cada equipo en casa y fuera\n",
    "    def mapper(self, _, line):\n",
    "        # Extraemos las columnas necesarias (asegurándonos de saltar las cabeceras)\n",
    "        _, _, _, home_team, away_team, home_goals, away_goals, *rest = line.split(',')\n",
    "        \n",
    "        # Si la línea es la cabecera, no la procesamos\n",
    "        if home_team == \"HomeTeam\":\n",
    "            return\n",
    "\n",
    "        # Emitimos los goles marcados por cada equipo\n",
    "        yield home_team, int(home_goals)  # Goles de casa\n",
    "        yield away_team, int(away_goals)  # Goles de visitante\n",
    "        \n",
    "    # Combiner: Suma de los goles de cada equipo (para reducir la cantidad de datos enviados al reducer)\n",
    "    def combiner(self, team, goals):\n",
    "        yield team, sum(goals)\n",
    "    \n",
    "    # Reducer: Sumamos los goles por equipo y encontramos los equipos con más y menos goles\n",
    "    def reducer(self, team, goals):\n",
    "        total_goals = sum(goals)\n",
    "        yield None, (team, total_goals)\n",
    "\n",
    "    # Reducer final para ordenar los resultados y calcular la diferencia de goles\n",
    "    def reducer_final(self, _, teams_goals):\n",
    "        sorted_teams = sorted(teams_goals, key=lambda t: t[1], reverse=True)\n",
    "        \n",
    "        # El primer equipo tiene la mayor cantidad de goles y el último la menor cantidad\n",
    "        max_goals_team = sorted_teams[0]\n",
    "        min_goals_team = sorted_teams[-1]\n",
    "        \n",
    "        # Calculamos la diferencia de goles\n",
    "        diff_goals = max_goals_team[1] - min_goals_team[1]\n",
    "        \n",
    "        yield f\"{max_goals_team[0]} vs {min_goals_team[0]}\", f'diferencia de goles {diff_goals}'\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper,\n",
    "                   combiner=self.combiner,\n",
    "                   reducer=self.reducer),\n",
    "            MRStep(reducer=self.reducer_final)\n",
    "        ]\n",
    "         \n",
    "if __name__=='__main__':\n",
    "    LaLigaMR.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod ugo+x laligaMRDiff.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /app/hadoop-3.3.1/bin...\n",
      "Found hadoop binary: /app/hadoop-3.3.1/bin/hadoop\n",
      "Using Hadoop version 3.3.1\n",
      "Looking for Hadoop streaming jar in /app/hadoop-3.3.1...\n",
      "Found Hadoop streaming jar: /app/hadoop-3.3.1/share/hadoop/tools/lib/hadoop-streaming-3.3.1.jar\n",
      "Creating temp directory /tmp/laligaMRDiff.root.20250107.151438.686876\n",
      "uploading working dir files to hdfs:///user/root/tmp/mrjob/laligaMRDiff.root.20250107.151438.686876/files/wd...\n",
      "Copying other local files to hdfs:///user/root/tmp/mrjob/laligaMRDiff.root.20250107.151438.686876/files/\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [/tmp/hadoop-unjar4790187191258124331/] [] /tmp/streamjob887234798919385259.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.20.0.2:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.20.0.2:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1736255952742_0039\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1736255952742_0039\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1736255952742_0039\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1736255952742_0039/\n",
      "  Running job: job_1736255952742_0039\n",
      "  Job job_1736255952742_0039 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1736255952742_0039 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/laligaMRDiff.root.20250107.151438.686876/step-output/0000\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=176270\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=430\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=626\n",
      "\t\tFILE: Number of bytes written=837060\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=176578\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=430\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=3190784\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1274880\n",
      "\t\tTotal time spent by all map tasks (ms)=3116\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=3116\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1245\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1245\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=3116\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1245\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=880\n",
      "\t\tCombine input records=760\n",
      "\t\tCombine output records=40\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=124\n",
      "\t\tInput split bytes=308\n",
      "\t\tMap input records=381\n",
      "\t\tMap output bytes=9500\n",
      "\t\tMap output materialized bytes=632\n",
      "\t\tMap output records=760\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=349843456\n",
      "\t\tPeak Map Virtual memory (bytes)=2556563456\n",
      "\t\tPeak Reduce Physical memory (bytes)=205033472\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2559254528\n",
      "\t\tPhysical memory (bytes) snapshot=858951680\n",
      "\t\tReduce input groups=20\n",
      "\t\tReduce input records=40\n",
      "\t\tReduce output records=20\n",
      "\t\tReduce shuffle bytes=632\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=80\n",
      "\t\tTotal committed heap usage (bytes)=740818944\n",
      "\t\tVirtual memory (bytes) snapshot=7669694464\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 2 of 2...\n",
      "  packageJobJar: [/tmp/hadoop-unjar589755759533548658/] [] /tmp/streamjob7343930003338460448.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.20.0.2:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.20.0.2:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1736255952742_0040\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1736255952742_0040\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1736255952742_0040\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1736255952742_0040/\n",
      "  Running job: job_1736255952742_0040\n",
      "  Job job_1736255952742_0040 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1736255952742_0040 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/laligaMRDiff.root.20250107.151438.686876/output\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=645\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=476\n",
      "\t\tFILE: Number of bytes written=835197\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=967\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=49\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=2609152\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1315840\n",
      "\t\tTotal time spent by all map tasks (ms)=2548\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=2548\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1285\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1285\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=2548\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1285\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=730\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=99\n",
      "\t\tInput split bytes=322\n",
      "\t\tMap input records=20\n",
      "\t\tMap output bytes=430\n",
      "\t\tMap output materialized bytes=482\n",
      "\t\tMap output records=20\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=297779200\n",
      "\t\tPeak Map Virtual memory (bytes)=2549166080\n",
      "\t\tPeak Reduce Physical memory (bytes)=202932224\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2555588608\n",
      "\t\tPhysical memory (bytes) snapshot=797536256\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce input records=20\n",
      "\t\tReduce output records=1\n",
      "\t\tReduce shuffle bytes=482\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=40\n",
      "\t\tTotal committed heap usage (bytes)=730333184\n",
      "\t\tVirtual memory (bytes) snapshot=7649533952\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///user/root/tmp/mrjob/laligaMRDiff.root.20250107.151438.686876/output\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/laligaMRDiff.root.20250107.151438.686876/output...\n",
      "\"Real Madrid vs Alaves\"\t\"diferencia de goles 49\"\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/laligaMRDiff.root.20250107.151438.686876...\n",
      "Removing temp directory /tmp/laligaMRDiff.root.20250107.151438.686876...\n"
     ]
    }
   ],
   "source": [
    "! python3 laligaMRDiff.py -r hadoop laliga2122.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.- Calcula la racha de los últimos cinco partidos de cada equipo en la clasificación final de La Liga en la temporada 2021/2022.\n",
    "\n",
    "[Observa](https://www.google.com/search?q=clasificacion+liga+2021+2022&oq=clasificacion+liga+2021+2022#sie=lg) que las últimas columnas de la clasificación muestran cuál ha sido el resultado de los últimos 5 partidos de cada equipo.\n",
    "\n",
    "![clasificacion](./img/clasificacion.png)\n",
    "\n",
    "Se trata de que muestres la clasificación final junto con los resultados de los últimos 5 partidos. Este ejercicio es un poco más difícil y laborioso que los otros. Si usas `mrjob` probablemente te sea útil utilizar [ordenación secundaria por valor](https://mrjob.readthedocs.io/en/latest/job.html#secondary-sort), aunque también se puede resolver sin hacer uso de ella.\n",
    "\n",
    "Se espera este resultado:\n",
    "\n",
    "![solución 5](./img/5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este me ha costado bastante. He probado mil maneras y al final encontré una que funciona. No se me ordenaban los 5 ultimos resultados, me los ponia mezclados y más problemas que he tenido.\n",
    "Hay algunas partes del código que tengo que estudiarlas más, ya que no las entiendo del todo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting laligaMRLast5Matches.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile laligaMRLast5Matches.py\n",
    "#!/usr/bin/python3\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from datetime import datetime\n",
    "    \n",
    "class laligaMRLast5Matches(MRJob):\n",
    "\n",
    "    # Mapper: En esta etapa aún no hay clave (_), el valor lo recibimos en la variable line\n",
    "    def mapper_points(self, _, line):\n",
    "        # Por cada línea, esta se divide en los campos que forman las columnas\n",
    "        _, date, _, home_team, away_team, _, _, result, *rest = line.split(',')\n",
    "        \n",
    "        # Si es la cabecera no emitimos nada\n",
    "        if home_team == \"HomeTeam\":\n",
    "            return\n",
    "        \n",
    "        # Convertir la fecha para ordenar correctamente\n",
    "        date = datetime.strptime(date, \"%d/%m/%Y\").strftime(\"%Y/%m/%d\")\n",
    "        \n",
    "        # Emitimos la fecha y los puntos para cada equipo\n",
    "        if result == 'D':            \n",
    "            yield home_team, (date, 1)\n",
    "            yield away_team, (date, 1)\n",
    "        elif result == 'H':\n",
    "            yield home_team, (date, 3)\n",
    "            yield away_team, (date, 0)\n",
    "        else:\n",
    "            yield home_team, (date, 0)\n",
    "            yield away_team, (date, 3)\n",
    "            \n",
    "    def combiner_points(self, team, values):\n",
    "        # Combina los puntos y fechas por equipo\n",
    "        combined_points = list(values)\n",
    "        yield team, combined_points\n",
    "            \n",
    "    def reducer_points(self, team, values):\n",
    "        # Aplanamos y ordenamos por fecha\n",
    "        all_games = sorted([item for sublist in values for item in sublist], key=lambda x: x[0])\n",
    "\n",
    "        # Obtenemos los puntos de los últimos 5 partidos\n",
    "        last_five_points = [points for date, points in all_games[-5:]]\n",
    "        last_five_points.reverse()\n",
    "        # Calculamos el total de puntos\n",
    "        total_points = sum(points for date, points in all_games)\n",
    "\n",
    "        yield None, (team, total_points, last_five_points)\n",
    "    \n",
    "    def reducer_classification(self, _, points):\n",
    "        # Ordenamos los equipos por los puntos totales\n",
    "        yield None, sorted(points, key=lambda t: t[1], reverse=True)\n",
    "            \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_points,\n",
    "                   combiner=self.combiner_points,\n",
    "                   reducer=self.reducer_points),\n",
    "            MRStep(reducer=self.reducer_classification)\n",
    "        ]\n",
    "         \n",
    "if __name__=='__main__':\n",
    "    laligaMRLast5Matches.run()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod ugo+x laligaMRLast5Matches.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /app/hadoop-3.3.1/bin...\n",
      "Found hadoop binary: /app/hadoop-3.3.1/bin/hadoop\n",
      "Using Hadoop version 3.3.1\n",
      "Looking for Hadoop streaming jar in /app/hadoop-3.3.1...\n",
      "Found Hadoop streaming jar: /app/hadoop-3.3.1/share/hadoop/tools/lib/hadoop-streaming-3.3.1.jar\n",
      "Creating temp directory /tmp/laligaMRLast5Matches.root.20250107.151517.162001\n",
      "uploading working dir files to hdfs:///user/root/tmp/mrjob/laligaMRLast5Matches.root.20250107.151517.162001/files/wd...\n",
      "Copying other local files to hdfs:///user/root/tmp/mrjob/laligaMRLast5Matches.root.20250107.151517.162001/files/\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [/tmp/hadoop-unjar5092812142297854453/] [] /tmp/streamjob8165514791368388460.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.20.0.2:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.20.0.2:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1736255952742_0041\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1736255952742_0041\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1736255952742_0041\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1736255952742_0041/\n",
      "  Running job: job_1736255952742_0041\n",
      "  Job job_1736255952742_0041 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1736255952742_0041 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/laligaMRLast5Matches.root.20250107.151517.162001/step-output/0000\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=176270\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=770\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=15146\n",
      "\t\tFILE: Number of bytes written=866499\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=176594\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=770\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=3103744\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1318912\n",
      "\t\tTotal time spent by all map tasks (ms)=3031\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=3031\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1288\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1288\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=3031\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1288\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=850\n",
      "\t\tCombine input records=760\n",
      "\t\tCombine output records=40\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=129\n",
      "\t\tInput split bytes=324\n",
      "\t\tMap input records=381\n",
      "\t\tMap output bytes=21660\n",
      "\t\tMap output materialized bytes=15152\n",
      "\t\tMap output records=760\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=306429952\n",
      "\t\tPeak Map Virtual memory (bytes)=2555351040\n",
      "\t\tPeak Reduce Physical memory (bytes)=194170880\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2553192448\n",
      "\t\tPhysical memory (bytes) snapshot=805539840\n",
      "\t\tReduce input groups=20\n",
      "\t\tReduce input records=40\n",
      "\t\tReduce output records=20\n",
      "\t\tReduce shuffle bytes=15152\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=80\n",
      "\t\tTotal committed heap usage (bytes)=728760320\n",
      "\t\tVirtual memory (bytes) snapshot=7663722496\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 2 of 2...\n",
      "  packageJobJar: [/tmp/hadoop-unjar4641685792790104566/] [] /tmp/streamjob1601566675614724453.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.20.0.2:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.20.0.2:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1736255952742_0042\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1736255952742_0042\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1736255952742_0042\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1736255952742_0042/\n",
      "  Running job: job_1736255952742_0042\n",
      "  Job job_1736255952742_0042 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1736255952742_0042 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/laligaMRLast5Matches.root.20250107.151517.162001/output\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=1155\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=696\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=816\n",
      "\t\tFILE: Number of bytes written=836225\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1493\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=696\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=2594816\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1340416\n",
      "\t\tTotal time spent by all map tasks (ms)=2534\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=2534\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1309\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1309\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=2534\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1309\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=760\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=104\n",
      "\t\tInput split bytes=338\n",
      "\t\tMap input records=20\n",
      "\t\tMap output bytes=770\n",
      "\t\tMap output materialized bytes=822\n",
      "\t\tMap output records=20\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=339730432\n",
      "\t\tPeak Map Virtual memory (bytes)=2555781120\n",
      "\t\tPeak Reduce Physical memory (bytes)=200974336\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2553610240\n",
      "\t\tPhysical memory (bytes) snapshot=842137600\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce input records=20\n",
      "\t\tReduce output records=1\n",
      "\t\tReduce shuffle bytes=822\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=40\n",
      "\t\tTotal committed heap usage (bytes)=748158976\n",
      "\t\tVirtual memory (bytes) snapshot=7659757568\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///user/root/tmp/mrjob/laligaMRLast5Matches.root.20250107.151517.162001/output\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/laligaMRLast5Matches.root.20250107.151517.162001/output...\n",
      "null\t[[\"Real Madrid\", 86, [1, 1, 3, 0, 3]], [\"Barcelona\", 73, [0, 1, 3, 3, 3]], [\"Ath Madrid\", 71, [3, 1, 3, 3, 0]], [\"Sevilla\", 70, [3, 1, 1, 1, 1]], [\"Betis\", 65, [1, 3, 3, 0, 1]], [\"Sociedad\", 62, [0, 3, 3, 0, 1]], [\"Villarreal\", 59, [3, 0, 3, 1, 0]], [\"Ath Bilbao\", 55, [0, 3, 0, 1, 3]], [\"Valencia\", 48, [3, 1, 0, 1, 1]], [\"Osasuna\", 47, [0, 0, 1, 1, 1]], [\"Celta\", 46, [0, 3, 0, 3, 1]], [\"Vallecano\", 42, [0, 0, 0, 1, 1]], [\"Espanol\", 42, [1, 1, 0, 1, 0]], [\"Elche\", 42, [3, 0, 0, 0, 1]], [\"Mallorca\", 39, [3, 3, 1, 0, 0]], [\"Getafe\", 39, [0, 1, 1, 1, 1]], [\"Cadiz\", 39, [3, 1, 0, 3, 1]], [\"Granada\", 38, [1, 0, 3, 3, 1]], [\"Levante\", 35, [3, 3, 0, 3, 1]], [\"Alaves\", 31, [0, 0, 3, 0, 3]]]\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/laligaMRLast5Matches.root.20250107.151517.162001...\n",
      "Removing temp directory /tmp/laligaMRLast5Matches.root.20250107.151517.162001...\n"
     ]
    }
   ],
   "source": [
    "! python3 laligaMRLast5Matches.py -r hadoop laliga2122.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
